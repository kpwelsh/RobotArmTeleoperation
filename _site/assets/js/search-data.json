{"0": {
    "doc": "ARAT",
    "title": "Action Research Arm Test",
    "content": "In addition to the tests that cover specific manipulation modalities, the Action Research Arm Test (ARAT) was included to cover a wide range of general manipulation skills. This test was designed for use with patients with stroke, brain injury, multiple sclerosis, and Parkinson’s disease and asks subjects to go through a range of manipulations including placing a ball on a dimpled platform, stacking rectangular blocks on a shelf, sliding cylinders onto small posts, and poring liquid from one cup to another. When performed with human hands, the test includes a few other similar tasks, which were cut from this implementation because the size of the robot end-effectors and each arm’s kinematic workspace require the tasks to be spaced somewhat less densely. The figure below shows the configuration of the task in this simulation. This task is evaluated on a somewhat subjective rating scale for each sub-task. On each sub-task, the user scores 0 points if there is no movement, 1 point if the task is partially complete, 2 points if the task is completed but takes abnormally long time, and a maximum of 3 points if the task is performed normally. Crucially, the definition of this scoring system is relative to a typically functioning adult. Instead of this rating scale, each sub-task was given its own objective measure. For most sub-tasks, this is a binary value representing whether or not it was completed. For some, like the poring task, the number of completed components are counted and used as a fractional score. ",
    "url": "http://localhost:4000/task_models/arat.html#action-research-arm-test",
    "relUrl": "/task_models/arat.html#action-research-arm-test"
  },"1": {
    "doc": "ARAT",
    "title": "Technical Details",
    "content": "The ARAT is implemented in the ComplexDexterity class, and makes use of the ContainerCounter, PoseMatcher, and the CompletelyContainedColliderTracker components. ",
    "url": "http://localhost:4000/task_models/arat.html#technical-details",
    "relUrl": "/task_models/arat.html#technical-details"
  },"2": {
    "doc": "ARAT",
    "title": "PoseMatcher",
    "content": "This component offers a convenient way for task designers specify a pose matching task in the Unity Editor. It exposes a few editor buttons that allow the designer to set the desired pose by moving the object instead of typing in numbers by hand. Additionally, it affords an error tolerance threshold for both rotation and translation. To use this component, simply attach it to a game object, position it where you want it to start, hit the “Set Start Pose” button to save the current pose, position it where you want it to be placed by the user, hit the “Set Goal Pose” button (or alternatively edit the target pose properties directly), and then hit the “Reset Pose” button to set the object back to the start pose. ",
    "url": "http://localhost:4000/task_models/arat.html#posematcher",
    "relUrl": "/task_models/arat.html#posematcher"
  },"3": {
    "doc": "ARAT",
    "title": "ColliderTracker",
    "content": "Unity supports Trigger Colliders, which expose the event handling typically offered by a physics simulation without actually influencing the physics. This can be very useful for detecting if certain kinds of objects are present in a region and is used by the StickyGripper class to detect grabable things. In this case, the ColliderTracker exposes an interface keeping track of which colliders intersect and subsequently filtering them down to meet certain criteria. This is used as the base class for the following two components, both of which are implemented by adding their own filter criteria on top of the options of the ColliderTracker. ",
    "url": "http://localhost:4000/task_models/arat.html#collidertracker",
    "relUrl": "/task_models/arat.html#collidertracker"
  },"4": {
    "doc": "ARAT",
    "title": "CompletelyContainedColliderTracker",
    "content": "This component applies an additional filter on top of whether or not a collider intersects with the detection area. If both the detection area and the detected collider are BoxColliders, this component additionally checks whether or not the box is completely contained within the detection area. In the ARAT simulation, this is used to determine if the blocks are stacked on the shelves or not. | | . | Shelf stacking area. The green rectangle boxes need to be placed on the center shelf. The wireframe box shown here (not visible during operation) represents the detection area that the blocks need to be contained within to count. | . ContainerCounter1 . In addition to testing for convex objects within a another convex object, it can be useful to test whether or not a convex object is contained within a convex hull induced by the empty space in a concave object. Specifically, this is used for counting the number of balls inside of a cup. While this is somewhat complex in general, this implementation was simplified through several assumptions. Firstly that both the object being checked for containment and the bounds of the container are solid physical objects that do not intersect. Secondly, that the container is a concave object that can be made convex through the addition of a single convex shape (i.e. there is only one interior of the container and it is convex). And lastly that it is acceptable to give a non-deterministic answer if the object is partially contained (this limitation can be mitigated by combining the result with the use of other containment checks). With these assumptions, the algorithm is straightforward. Given a container and an object, we choose a point inside of the container and a point inside of the object. Then, we simply cast a ray from one to the other. If the ray intersects with our container, the object is guaranteed to be not contained. Similarly, if the object is completely contained, the ray is guaranteed to not intersect with the container. However, if the ray does not intersect with the container, it is possible that another choice of points cause an intersection. As such, this algorithm has a false positive rate if the object is partially contained. The Count function performs two checks. Firstly that the collider in question intersects with all of the specified containment regions. In the case of the cup, this is a single cylinder. This check is needed to bound the convex interior of the cup. HashSet&lt;Collider&gt; colliders = new HashSet&lt;Collider&gt;(Boundaries[0].Colliders); foreach (var boundary in Boundaries) { colliders.IntersectWith(boundary.Colliders); } . Secondly, it casts a ray from a point in the interior of the cup to a point in the interior of the detected collider. In doing so, it checks if the ray intersects with the cup model. If it does, then the detected collider is certainly not contained in the cup. If it does not, then the detected collider may be contained in the cup in general, and is contained given the previous assumptions. foreach (var collider in colliders) { if (!filter(collider)) continue; Ray ray = new Ray(origin, collider.transform.position); List&lt;RaycastHit&gt; hits = new List&lt;RaycastHit&gt;(); // Assuming that the origin of the transform is within the collider foreach (var hit in Physics.RaycastAll(ray, (collider.transform.position - origin).magnitude)) { // If we hit ourself, then we are done looking. if (hit.collider.gameObject.HasParent(gameObject)) break; // Otherwise, if we hit the collider we are looking for, then we count it as inside. if (hit.collider == collider) { count++; break; } } } . | The current design of the cup model allows this to be solved much more simply by doing a cylinder containment check. However, this was not the case with previous cup models, and this section is written with general models in mind. &#8617; . | . ",
    "url": "http://localhost:4000/task_models/arat.html#completelycontainedcollidertracker",
    "relUrl": "/task_models/arat.html#completelycontainedcollidertracker"
  },"5": {
    "doc": "ARAT",
    "title": "ARAT",
    "content": " ",
    "url": "http://localhost:4000/task_models/arat.html",
    "relUrl": "/task_models/arat.html"
  },"6": {
    "doc": "Box and Block",
    "title": "Box and Block",
    "content": "Because pick-and-place applications represent such a large part of robotics, we wanted one of the tasks to be primarily focused on simple pick-and-place. For this, we looked to the Box and Blocks Test that was developed to evaluate gross manual dexterity in populations with upper extremity impairment or severe cognitive impairment. During this task, shown below, the subject is asked to move as many blocks as they can from one box to another in a given time. To count as a moved block, the subject must carry the blocks past the barrier in the center at which point they are allowed to release the block into the other box. The object metric used is simply the number of moved blocks during the time frame. While the human test design recommends a time limit of 1 minute, we found that under certain configurations users were only able to move a small number of blocks in that time and so to ensure appropriate performance resolution, we use times as large as 3 minutes. ",
    "url": "http://localhost:4000/task_models/box_and_block.html",
    "relUrl": "/task_models/box_and_block.html"
  },"7": {
    "doc": "Box and Block",
    "title": "Technical Details",
    "content": "The Box and Block task represents the simplest task implementation, technically speaking. The BoxDexterity class implements the Task interface and keeps track of the objective metric by using the ColliderCounter component to count how many blocks are in the target box. ",
    "url": "http://localhost:4000/task_models/box_and_block.html#technical-details",
    "relUrl": "/task_models/box_and_block.html#technical-details"
  },"8": {
    "doc": "Build Settings",
    "title": "Building the Project",
    "content": "While it is possible to run the simulations directly through the editor’s play button, it is likely that you will want to actually build and deploy it at some point. To do so, first read this page on building Unity projects. Specific to this project, there are a few things you might want to modify. Firstly, using the File-&gt;Build Settings menu, you can switch the build target between WebGL to host on a web server and run in the browser and a native build to run as a desktop application. Alternatively, if you do not have a VR system at hand, or you want to disable VR integration temporarily for testing, you can do so by going to the Player Settings (located on the Build Settings window) and change whether or not you start VR on initialization. When enabled, Unity will wait for the VR backend before starting the simulation. ",
    "url": "http://localhost:4000/behind_the_scenes/build_settings.html#building-the-project",
    "relUrl": "/behind_the_scenes/build_settings.html#building-the-project"
  },"9": {
    "doc": "Build Settings",
    "title": "Deploying",
    "content": "If you are building for a WebGL target, the WebAssembly, Javascript, and HTML output can be found in the build directory. Then, you can serve the contents at an appropriate web endpoint. keep in mind that this project contains multiple plugins that have separate WebGL and native DLL versions. Failure to install these correctly can result in discrepancies between what you see in the Unity Editor and what is run in the browser. ",
    "url": "http://localhost:4000/behind_the_scenes/build_settings.html#deploying",
    "relUrl": "/behind_the_scenes/build_settings.html#deploying"
  },"10": {
    "doc": "Build Settings",
    "title": "Build Settings",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/build_settings.html",
    "relUrl": "/behind_the_scenes/build_settings.html"
  },"11": {
    "doc": "Data Logging",
    "title": "Data Logging",
    "content": "Included in this project is abstract interface for secure database access (SecureDatabase&lt;T&gt;). This supports a few simple operations: . | Authorize - Provide credentials to the database and determine if it acess is allowed. | IsAuthorized - Returns whether or not the current process is authorized. | ReadRecord, WriteRecord, ListRecords - Peforms basic database operations (requires authorization). | . To use this for datalogging, you need to either make use of one of the existing implementations of SecureDatabase, or make your own. When provided with a serializable type T, the database will automatically serialize and deserialize the records for you. The following is an example of how to use this interface to log TaskRecord (sourced from SerializationTest.cs): ... taskDb = new RemoteDatabase&lt;TaskRecord&gt;(Url); SystemManager.StartScene(); ... // Some time later SystemManager.EndScene(); // Attempt to authorize with some credentials taskDb.Authorize(\"hcilab\").ContinueWith(async (result) =&gt; { if (result.Result) { // If we are authorized, then write the record. await taskDb.WriteRecord(SystemManager.taskRecord, SystemManager.taskRecord.Name); // (Unnecessary) Read the record from the DB var taskRecord = await taskDb.ReadRecord(SystemManager.taskRecord.Name); // And replay it. SystemManager.ReplayScene(taskRecord); } }); . ",
    "url": "http://localhost:4000/modification_workflow/data_logging.html",
    "relUrl": "/modification_workflow/data_logging.html"
  },"12": {
    "doc": "Data Logging",
    "title": "LocalDatabase",
    "content": "The local implementation stores the data on disk using the C# OS API. If running natively, this does not requrie authorization. It only requires the path to the logging directory. ",
    "url": "http://localhost:4000/modification_workflow/data_logging.html#localdatabase",
    "relUrl": "/modification_workflow/data_logging.html#localdatabase"
  },"13": {
    "doc": "Data Logging",
    "title": "RemoteDatabase",
    "content": "The remote implementation communicates with an HTTP server, which provides the necessary API implementation. To manage authentication, this class uses JSON Web Tokens, and relies on a secure HTTPS connection with the server to offer appropriate security. The server is required to support the following API endpoints: . | Authorize - POST /login . REQUEST . { headers : { Accept : application/json, ContentType : application/json }, body : { username : &lt;credentials&gt; } } . RESPONSE . { ... body : { access_token : &lt;JWT&gt; } } . | ReadRecord - GET /record . REQUEST . { headers : { Accept : application/json, ContentType : application/json, Authorization : Bearer &lt;JWT&gt; }, body : { name : &lt;record identifier&gt; } } . RESPONSE . { ... body : &lt;JSON Serialized Record&gt; } . | WriteRecord - POST /record . REQUEST . { headers : { Accept : application/json, ContentType : application/json Authorization : Bearer &lt;JWT&gt; }, body : { name : &lt;record identifier&gt;, data : &lt;JSON Serialized Record&gt; } } . | ListRecords - GET /listRecords . REQUEST . { headers : { Accept : application/json, ContentType : application/json Authorization : Bearer &lt;JWT&gt; } } . RESPONSE . { ... body : { names : [&lt;record identifier&gt;, ...] } } . | . An example of such a server can be found here. (Warning, this example does not contain a transport layer security (TLS) protocol, and requires a secure server to be placed in front of it.) . ",
    "url": "http://localhost:4000/modification_workflow/data_logging.html#remotedatabase",
    "relUrl": "/modification_workflow/data_logging.html#remotedatabase"
  },"14": {
    "doc": "End Effectors",
    "title": "Usage",
    "content": "The EndEffector component is an abstract class that can be inherited and implemented by a gripper. Examples of this are the StickGripper, which can be used to simulate realistic grasping, the PendEffector, which acts a marker for surface painting, and the InspectionCamera, which can take images from the point of view of the robot arm. When creating a new end effector, you can specify the IKPoint and handle activation triggers by overriding the Activate functions. When the IKPoint is specified, the robot arm will automatically track to match that point with the commanded pose instead of the default pose located at the base of the hand. | Example Gripper End Effector | . | | . ",
    "url": "http://localhost:4000/behind_the_scenes/components/end_effectors.html#usage",
    "relUrl": "/behind_the_scenes/components/end_effectors.html#usage"
  },"15": {
    "doc": "End Effectors",
    "title": "End Effectors",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/end_effectors.html",
    "relUrl": "/behind_the_scenes/components/end_effectors.html"
  },"16": {
    "doc": "Feedback Devices",
    "title": "Usage",
    "content": "This project currently supports several visual feedback modalities, and opporunity to add others. They can currently be broken into two categories – VR and Non-VR. Initializing and providing the correct feedback modality is responsibility of the active InputRig component. In the current implementation, only the VRInputRig is used. However, for non-VR applications, the MonitorInputRig supports rendering a limited set of feedback sources directly to the monitor. The MonitorInputRig supports static single-cam feedback as well as static multi-cam feedback. The VRInputRig supports both of these modalities in addition to both stereo and mono VR. ",
    "url": "http://localhost:4000/behind_the_scenes/components/feedback_devices.html#usage",
    "relUrl": "/behind_the_scenes/components/feedback_devices.html#usage"
  },"17": {
    "doc": "Feedback Devices",
    "title": "Technical Details",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/feedback_devices.html#technical-details",
    "relUrl": "/behind_the_scenes/components/feedback_devices.html#technical-details"
  },"18": {
    "doc": "Feedback Devices",
    "title": "VRInputRig",
    "content": "When the user is wearing a VR headset, the simplest form of visual feedback to provide is stereoscopic VR. This is supported out of the box in a Unity environemnt for both native builds and WebGL builds by making use of the WebXR Exporter. This component provides an XR Rig which connects to the running VR backend using the WebXR communication specification. It similarly provides gameobjects that track the user’s hands and methods for reading user input. Mono VR . To truly enable monoscopic vision, it is necessary to feed the same image to both eyes. In lieu of making modifications to core XR camera modules to change the multi-eye render process, this project implements an external solution that works for general VR headset drivers. This is acheived by effectively enlarging the scene and placing it very far away from the eyes such that the interpupillary distance is effectively 0 when computing the perspective shift. To get this effect without having to disrupt the physics, we instead switch from rendering the scene directly to the eye cameras, and instead render it once from a perspective between the eyes and project it on a head-mounted screen that is placed at a signficant distance. | | . | An example of the mono rendering strategy. The eye cameras don’t render the real scene and can only see the large projection in the distance. The viewpoint of the mono cam tracking the head motion is shown in the picture-in-picture view for illustration purposes. Similarly, the size and distance of the projected screen was reduced so that all of the elements in the scene could be seen at once. | . Static Camera Feedback . When using the static camera feedback modalities, the control modality is unaffected, but the user is placed in small virtual room with a monitor on top of it. In reality, the user is still technically collocated with the robot, but that scene layer is not rendered and this monitor room is rendered instead. when initializing the task, the active InputRig’s ObserveScene method will be called to look through the active task and arrange the video feeds from the static cameras found in the scene to display to the user. The VRInputRig does this by stitching the video feeds together with the Rectangle Packing component and rendering the result to a RenderTexture. ",
    "url": "http://localhost:4000/behind_the_scenes/components/feedback_devices.html#vrinputrig",
    "relUrl": "/behind_the_scenes/components/feedback_devices.html#vrinputrig"
  },"19": {
    "doc": "Feedback Devices",
    "title": "Feedback Devices",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/feedback_devices.html",
    "relUrl": "/behind_the_scenes/components/feedback_devices.html"
  },"20": {
    "doc": "IK Plugins",
    "title": "Usage",
    "content": "The provided IK Solving Plugins are made available via the NativeRobotController component. In general, an implementation of RobotController should be present on each robot arm. The controller allows for the specification of a hand joint (where end effectors will be attached to), URDF model, initial joint configuration for seeding the solver, a pointer to the IK Target that should be tracked, and the name of the hand joint to give to the IK Plugin. While running, the controller will transform the IK Target into its local frame, transform it into a canonical right handed coordinate frame for the IK plugin, and command the positions of the joints. The dynamics of each joint can be controlled on a global level as well to modify behavior to be more or less sluggish and responsive. ",
    "url": "http://localhost:4000/behind_the_scenes/components/ik_plugins.html#usage",
    "relUrl": "/behind_the_scenes/components/ik_plugins.html#usage"
  },"21": {
    "doc": "IK Plugins",
    "title": "Technical Details",
    "content": "The core IK solver and Unity robot controller plugin is in 3 components that is built off of the RobotIKBase rust package. graph TD; Base[RobotIKBase]--&gt;WASM[robot_ik_wasm.js]; Base--&gt;Native[robot_ik_native.dll]; Native--&gt;IKInterface[IKInterface.cs]; WASM--&gt;IKInterface[IKInterface.cs]; The core, meaningful code is written in Rust, and is subsequently compiled for the native machine (to run the simulation through the Unity Editor) and for WebAssembly to run in the browser. For detailed instructions on how to correctly install and build these components, see the Installation page. IKInterface.cs conditionally compiles to look for one end point or the other depending on the build target. ",
    "url": "http://localhost:4000/behind_the_scenes/components/ik_plugins.html#technical-details",
    "relUrl": "/behind_the_scenes/components/ik_plugins.html#technical-details"
  },"22": {
    "doc": "IK Plugins",
    "title": "IKInterface",
    "content": "The IKInterface class serves to safely abstract which IK plugin is being used behind the scenes. It leverages some straightforward conditional compiling to either link to a native library, or one provided by a JS interface. public class IKInterface { #if UNITY_EDITOR [DllImport(\"robot_ik_native\", EntryPoint = \"new_solver\", CallingConvention = CallingConvention.Cdecl)] unsafe private static extern void* new_solver(string urdf, string ee_frame); [DllImport(\"robot_ik_native\", EntryPoint = \"solve\", CallingConvention = CallingConvention.Cdecl)] unsafe private static extern bool solve(void* solver_ptr, float* current_q, float[] target, float* q); [DllImport(\"robot_ik_native\", EntryPoint = \"set_self_collision\", CallingConvention = CallingConvention.Cdecl)] unsafe private static extern bool set_self_collision(void* solver_ptr, bool self_collision_enabled); [DllImport(\"robot_ik_native\", EntryPoint = \"deallocate\", CallingConvention = CallingConvention.Cdecl)] unsafe private static extern void deallocate(void* solver_ptr); #elif UNITY_WEBGL [DllImport(\"__Internal\")] unsafe private static extern void* new_solver(string urdf, string ee_frame); [DllImport(\"__Internal\")] unsafe private static extern bool solve(void* solver_ptr, float[] current_q, float[] target, float[] q); [DllImport(\"__Internal\")] unsafe private static extern bool set_self_collision(void* solver_ptr, bool self_collision_enabled); [DllImport(\"__Internal\")] unsafe private static extern void deallocate(void* solver_ptr); #endif ... } . In addition to wrapping the function end-points, it ensures that the memory allocated by the IK Solver is cleaned up when the IKInterface object is destroyed... ~IKInterface() { unsafe { deallocate(solver_ptr); } } ... ",
    "url": "http://localhost:4000/behind_the_scenes/components/ik_plugins.html#ikinterface",
    "relUrl": "/behind_the_scenes/components/ik_plugins.html#ikinterface"
  },"23": {
    "doc": "IK Plugins",
    "title": "RobotIKNative",
    "content": "The native plugin exposes the RobotIKBase rust package to Unity via a C interface. For details on how FFI’s in Rust work, check out the Rustonomicon. Code for this repo can be found here. Unity can load native plugins from dynamic libraries if they exist in the Assets\\Plugins folder. ",
    "url": "http://localhost:4000/behind_the_scenes/components/ik_plugins.html#robotiknative",
    "relUrl": "/behind_the_scenes/components/ik_plugins.html#robotiknative"
  },"24": {
    "doc": "IK Plugins",
    "title": "RobotIKWASM",
    "content": "While Unity support bundling a mix of C++, C# and JS directly into the WebAssembly build, it doesn’t (as of writing) support native inclusion of WebAssembly files. So the WebAssembly version of the plugin has three sub-componets. Firstly, the actual plugin is compiled from the RobotIKWASM Rust package. This produces a .wasm file and a js file to interface with it. Excerpt from the generated file, robot_ik_wasm.js: ... /** * @param {string} urdf * @param {string} ee_frame * @returns {number} */ export function new_solver(urdf, ee_frame) { var ptr0 = passStringToWasm0(urdf, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc); var len0 = WASM_VECTOR_LEN; var ptr1 = passStringToWasm0(ee_frame, wasm.__wbindgen_malloc, wasm.__wbindgen_realloc); var len1 = WASM_VECTOR_LEN; var ret = wasm.new_solver(ptr0, len0, ptr1, len1); return ret; } ... This JS layer handles the memory management, data type transformation, and invokation of the WASM code. One thing to note is that, just like the native version, the wasm library call actually returns a *const IKSolver. However, we take advantage of the duplicity of pointers and integers and the permissiveness of JavaScript to simply reinterpret the bytes as a pointer when we pass them to the wasm code in the future. In addition to the layer from wasm &lt;-&gt; JS, there is another layer, located at Assets\\Plugins\\robot_ik_js_interface.jslib. This uses Unity API to transform the data between C# and JS as well as expose the libraries to the Unity WASM compiler. However, because the actual robot IK driver can’t be built with the rest of the project, it points the API to a placeholder object, RobotIK. The last piece of the puzzle is in Assets\\Plugins\\robot_ik_loader.jspre. Unity will execute *.jspre files before initialization of the core application. This one is used to declare the RobotIK placeholder, as well as dynamically load the actual IK plugin at runtime. Once these steps are in place, it functions much in the same way as the native plugin, with the exception that some data types (arrays, strings) need to be handled slightly differently. ",
    "url": "http://localhost:4000/behind_the_scenes/components/ik_plugins.html#robotikwasm",
    "relUrl": "/behind_the_scenes/components/ik_plugins.html#robotikwasm"
  },"25": {
    "doc": "IK Plugins",
    "title": "IK Plugins",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/ik_plugins.html",
    "relUrl": "/behind_the_scenes/components/ik_plugins.html"
  },"26": {
    "doc": "Task Models",
    "title": "Tasks",
    "content": "Each Task is comprised of several components: a timer, a collection of sub-tasks, an informational display, static camera positions, and the position of the robot arm. Each sub-task is required to contain an objective task completion measure and a name. The task scene will then automatically display the completion measures of all of the sub-tasks on the information display along with the current timer value. Much of the logic for a task is driven from the Unity transform hierarchy. For example, the task display and sub-task aggregation is handled by using the GetComponentsInChildren&lt;T&gt; function to find all of the necessary components without explicitly listing them in the editor. For details on how to create a new task, see Creating a New Task. ",
    "url": "http://localhost:4000/task_models/#tasks",
    "relUrl": "/task_models/#tasks"
  },"27": {
    "doc": "Task Models",
    "title": "Task Models",
    "content": " ",
    "url": "http://localhost:4000/task_models/",
    "relUrl": "/task_models/"
  },"28": {
    "doc": "Components",
    "title": "Components",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/",
    "relUrl": "/behind_the_scenes/components/"
  },"29": {
    "doc": "Modification Workflow",
    "title": "Modification Workflow",
    "content": " ",
    "url": "http://localhost:4000/modification_workflow/",
    "relUrl": "/modification_workflow/"
  },"30": {
    "doc": "Modified Components",
    "title": "Modified Components",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/modified_components/",
    "relUrl": "/behind_the_scenes/modified_components/"
  },"31": {
    "doc": "Overview",
    "title": "Editing the Project/Technical Details",
    "content": "Details on how to download, run and modify the Unity project for yourself as well as descriptions for all of the components involved can be found here. ",
    "url": "http://localhost:4000/#editing-the-projecttechnical-details",
    "relUrl": "/#editing-the-projecttechnical-details"
  },"32": {
    "doc": "Overview",
    "title": "Project Overview",
    "content": " ",
    "url": "http://localhost:4000/#project-overview",
    "relUrl": "/#project-overview"
  },"33": {
    "doc": "Overview",
    "title": "Motivation",
    "content": "Compared to autonomous robotics, developing a system for robotic teleoperation is typically seen as straightforward. By introducing a human operator the challenge of creating software that exhibits complex decision making, planning, and responsiveness are side stepped. However, simply introducing a human agent into the system does not result in a high performance system. Just last month, Daniel Rea and Stela Seo surveyed the field of teleoperation and concluded that effective teleoperation still requires highly trained expert users and calls for a . …[R]enewed focus in broad, user-centered research goals to improve teleoperation interfaces in everyday applications for non-experts…1 . While robotic teleoperation has the potential to achieve super-human performance, looking closesly at state of the art teleoperation systems suggests we have yet to achieve performance that is even comparable to a human. The following video shows several teleoperation systems, including a robot breaking the “break in case of emergency” glass with somewhat less urgency than optimal in an “emergency” situation. Comparing this to The Box and Block Test (BBT), a human dexterity and motor function test designed to evaluate individuals with a range of neurological diagnoses, it is clear that modern human + robot systems, even with expert users, have yet to surpass a lone human system in terms of absolute performance2. ",
    "url": "http://localhost:4000/#motivation",
    "relUrl": "/#motivation"
  },"34": {
    "doc": "Overview",
    "title": "Contents",
    "content": "This project was designed to further explore the performance gap between human and human + robot teleoperation performance. Developed in VR using Unity, this platform offers several useful components for measuring the impact of different performance factors: . | Realistic Task Models: . | Several robotic arm platforms with realistic dynamics | Task models based on real human dexterity evaluation tools . | Including variations that recreate the human tasks and those that are more kinematically favorable for robot manipulation | . | Realistic physical interactions | . | Robust Control Systems: . | An arm-agnostic realtime control system based on modern inverse kinematics algorithms | Knobs to control low level control variables . | Lowpass filters | Dynamic constraints (joint velocity and torque limits) | Latency | . | . | Variable Visual Feedback Modalities: . | Stereo VR | Mono VR | Single static camera | Muliple static cameras | . | Variable Input Device Support: . | 6DOF Unity XR/WebXR tracked devices | Interactive markers for keyboard and mouse | Unity supported gamepads (Xbox, PlayStation, Switch) | . | Web Deployability | Object Task Metrics . | Sub-task progress and completion time (WiP) | Remote task recording and replay (WiP) | . | . | Still Not Solved: a call for renewed focus on user-centered teleoperation interfaces &#8617; . | This is not to dismiss the ROI of adding a robot when there are other factors involved (e.g. safety or convenience). &#8617; . | . ",
    "url": "http://localhost:4000/#contents",
    "relUrl": "/#contents"
  },"35": {
    "doc": "Overview",
    "title": "Overview",
    "content": " ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"36": {
    "doc": "Behind the Scenes",
    "title": "Behind the Scenes",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/",
    "relUrl": "/behind_the_scenes/"
  },"37": {
    "doc": "Input Devices",
    "title": "Usage",
    "content": "Inlcuded in this project is support for three types of user input devices: Gamepads, 6DoF XR controllers, and mouse and keyboard. All three types of user interface implement the InputPoseProvider interface, the primary responsibility of which is to update its associated Transform to be the currently commanded pose. In general, each of the input systems can be placed in a scene and will give the user a way to command a pose. public class InputPoseProvider : MonoBehaviour { public Transform ControlPerspective = null; public Transform Pose { get; protected set; } public virtual void SetPoseFromExternal(Transform trans) { if (trans == null) return; transform.position = trans.position; transform.rotation = trans.rotation; } public virtual void Reset() { } } . In addition to the publically readable Pose property, it exposes methods for reseting or reinitializing input when a new task is initialized as well as setting the initial commanded pose to a task specific value. The ControlPerspective is used for differential control mode (i.e. a gamepad) that may prefer to control relative to the viewing perspective instead of relative to the world. Upon initialization, the InputProvider scans the transform tree for the correct type of pose provider depending on the system settings and throws an exception if it cannot find the specified provider, or it is not supported by the InputRig implementation. ",
    "url": "http://localhost:4000/behind_the_scenes/components/input_devices.html#usage",
    "relUrl": "/behind_the_scenes/components/input_devices.html#usage"
  },"38": {
    "doc": "Input Devices",
    "title": "Technical Details",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/input_devices.html#technical-details",
    "relUrl": "/behind_the_scenes/components/input_devices.html#technical-details"
  },"39": {
    "doc": "Input Devices",
    "title": "I6DOFInput",
    "content": "The 6DOF input modality is available to the VRInputRig uses a Tracker component to drive the output pose from the 6DOF controller pose. This additional transform layer is introduced to support a “clutching” control paradigm, where the user can temporarily disable the pose tracking to adjust their hand posture. ",
    "url": "http://localhost:4000/behind_the_scenes/components/input_devices.html#i6dofinput",
    "relUrl": "/behind_the_scenes/components/input_devices.html#i6dofinput"
  },"40": {
    "doc": "Input Devices",
    "title": "GamepadInput",
    "content": "The gamepad input component uses Unity’s Input System and Gamepad classes to read the value of state of the two joysticks, the shoulder buttons, and and the trigger buttons. On a fixed update interval, it translates the currently active buttons into a translational and rotational velocity using the values of GamepadInput.TranslationVelocity and GamepadInput.RotationalVelocity that are set can be set in the Unity Editor. Once the world-space translational velocity is computed it is rotated to match the current control frame if one is present. ",
    "url": "http://localhost:4000/behind_the_scenes/components/input_devices.html#gamepadinput",
    "relUrl": "/behind_the_scenes/components/input_devices.html#gamepadinput"
  },"41": {
    "doc": "Input Devices",
    "title": "Interactive Markers",
    "content": "The keyboard and mouse interface is modeled after the “Interactive Markers” control paradigm present in RViz, which provides an intuitive click and drag interface for adjusting a target pose. The Unity implementation provided behaves identically, except that the translational axes and the rotational axes are not shown simultaneously, but can be toggled with the left shift key. To control the dragging, an interactive marker contains 3 axes for both rotation and translation modeled as either rings and line segments, respectively. When the user clicks, a ray is casted into the scene. If it collides with one of the axis objects, that object is “selected,” and the ray is projected onto the axis by finding the closest point on the axis to the ray. This point is stored as the “anchor” point. Then, when the user moves their cursor, another ray is cast into the scene and again projected onto the selected axis geometry, and the position/rotation of the interactive marker is updated such that the “anchor” coincides with this new point, resulting in a slight shift. This tracking process continues while the user is still engaged with the marker by holding down the mouse button or until an extreme, discontinuous motion is detected (such motions can occur when using multi-cam interfaces). ",
    "url": "http://localhost:4000/behind_the_scenes/components/input_devices.html#interactive-markers",
    "relUrl": "/behind_the_scenes/components/input_devices.html#interactive-markers"
  },"42": {
    "doc": "Input Devices",
    "title": "Input Devices",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/input_devices.html",
    "relUrl": "/behind_the_scenes/components/input_devices.html"
  },"43": {
    "doc": "Inspection Camera",
    "title": "Usage",
    "content": "The inspection camera can be used as an end effector and capture images upon activation. It contains a camera component that renders to a RenderTexture named CameraBuffer. Either on Activation, or ad-hoc via the TakePic function, the camera will render its current view and store it in a Texture2D. This, along with the RectanglePacking utility can be used to create a collage of collected images. ",
    "url": "http://localhost:4000/behind_the_scenes/components/inspection_camera.html#usage",
    "relUrl": "/behind_the_scenes/components/inspection_camera.html#usage"
  },"44": {
    "doc": "Inspection Camera",
    "title": "Inspection Camera",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/inspection_camera.html",
    "relUrl": "/behind_the_scenes/components/inspection_camera.html"
  },"45": {
    "doc": "Installation",
    "title": "Installation",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/installation.html",
    "relUrl": "/behind_the_scenes/installation.html"
  },"46": {
    "doc": "Installation",
    "title": "Initializing the Unity Project",
    "content": "The minimum required to run this project is to . | clone this repo with git clone https://github.com/kpwelsh/RobotArmTeleoperation.git | Follow the steps here to add this project in Unity Hub. | Use Unity Hub to install version 2021.2.7. Some WebXR related components may not work with higher versions. Follow these instructions if that version is not immediately available in Unity Hub. | . Then, you should be able to open and run the Unity Editor to explore the project. See the Build Settings page for details on how to build and deploy the project for a WebGL or native target. ",
    "url": "http://localhost:4000/behind_the_scenes/installation.html#initializing-the-unity-project",
    "relUrl": "/behind_the_scenes/installation.html#initializing-the-unity-project"
  },"47": {
    "doc": "Installation",
    "title": "Building Robot Arm IK Controller",
    "content": "The code that determines the joint angles from a target end-effector pose written in Rust with interfaces for both native C libraries and WebAssembly. To build and deploy this Unity project as a web application, you will need the WebAssembly files. To build this project for a native target, or to run it in the editor, you will need to build the native rust package from source and install it into the appropriate Unity project folder. See the IK Plugins page for details on how these packages connect to Unity. Building From Source . Building the IK plugin from source is not always necessary. A DLL is included in the project, but may not be compatible with your operating system. If you experience issues, follow these instructions for building the Rust plugin from source. Building Rust . To compile the source code, you will need to have rust + cargo (rust package manager) installed. Follow the instructions on the Rust Homepage if you don’t already. Installing RobotIKNative . Clone the source code and compile with1: . git clone https://github.com/kpwelsh/RobotIKNative cd RobotIKNative cargo build --release . This will create dynamic library at target/release/robot_ik_native. Then, simply place this file in the Assets/Plugins folder in the Unity project. If you are replacing a robot_ik_native.dll file that is already there, you may need to restart the Unity Editor to be able remove the old one, since Unity caches these plugins and prevents the modification of DLLs while they are cached. Simply placing this DLL in this plugins folder with the correct name allows allows the code contained in IKInterface to find it. Installing RobotIKWASM . The RobotIKWASM js plugin and WASM binaries are included in this project. If they are not found in the Unity build folder, then copy the contents of /RobotIKWASM-Plugin at the project root directory to the Build folder in the build directory (default location: /build/Build). If you want to rebuild the robot_ik_wasm plugin, you will need to install wasm-pack with cargo install wasm-pack . Then, you can perform a similar build step as with the native plugin to clone and build the source code: . git clone https://github.com/kpwelsh/RobotIKWASM cd RobotIKWASM cargo build --release wasm-pack build --target web . Building for the web target generates js with the correct paradigm for easy inclusion into a Unity build. Finally, you can take robot_ik_wasm.js and robot_ik_wasm_bg.wasm and place them in the Build directory. Then, if everything went corretly, you should be able to build your Unity project with ctrl+b, ctrl+shift+b or from the build menu. ",
    "url": "http://localhost:4000/behind_the_scenes/installation.html#building-robot-arm-ik-controller",
    "relUrl": "/behind_the_scenes/installation.html#building-robot-arm-ik-controller"
  },"48": {
    "doc": "Installation",
    "title": "Running the Unity Project",
    "content": "This project makes use of multiple input devices (most notably VR), and typically compiles for a WebGL target. You can change the compile target under File-&gt;Build Settings. | Several of the dependencies in these trees point to git branches. Cargo will cache this kind of code. If there appears to be a mismatch between whats on GitHub and what your compiler is telling you, try updating the packages with cargo update. &#8617; . | . ",
    "url": "http://localhost:4000/behind_the_scenes/installation.html#running-the-unity-project",
    "relUrl": "/behind_the_scenes/installation.html#running-the-unity-project"
  },"49": {
    "doc": "New Robot",
    "title": "Creating a New Robot",
    "content": "Creating a new robot arm model is a straightforward process that requires just a few steps. ",
    "url": "http://localhost:4000/modification_workflow/new_robot.html#creating-a-new-robot",
    "relUrl": "/modification_workflow/new_robot.html#creating-a-new-robot"
  },"50": {
    "doc": "New Robot",
    "title": "1. Find a Model",
    "content": "The easiest way to create a new Unity robot is to start from an existing, well defined arm model. In general, you will need two things: a URDF file that describes the robot’s kinematics and dynamics, and a description folder that contains the necessary mesh files. Examples of this can be found in the existing Assets/RobotArm/RobotModels folder. Similarly, the Robot Models contains links to the source descriptions and URDF files for each arm. Show below is the recommended folder structure, with the URDF file at the top level next to the description file, along with other robot arm specific resources (e.g. a button thumbnail). ",
    "url": "http://localhost:4000/modification_workflow/new_robot.html#1-find-a-model",
    "relUrl": "/modification_workflow/new_robot.html#1-find-a-model"
  },"51": {
    "doc": "New Robot",
    "title": "2. Import the Robot and Add Components",
    "content": "Next, simply import the URDF file with the by invoking the URDF Importer from the context menu. This should create a game object in the scene. URDF Importer also attaches a controller component to the base of the model. For our purposes, this is typically insufficient. You can feel free to remove any and all URDF related components from the model at this point, and attach a RobotController to the base of the model and create an IKTarget for the end effector. With all controllers, you will need to fill out several fields about the robot and link certain child object. With the NativePluginController, you will also have to link a URDF text resource. Unity doesn’t recognize “*.urdf” files as text assets, so you will have to save the urdf as a “.txt” file before linking it to the controller. ",
    "url": "http://localhost:4000/modification_workflow/new_robot.html#2-import-the-robot-and-add-components",
    "relUrl": "/modification_workflow/new_robot.html#2-import-the-robot-and-add-components"
  },"52": {
    "doc": "New Robot",
    "title": "3. Testing the control",
    "content": "Often there small errors that occur during the import process, and some models may show up rotated from what you expected. To identify and debug these, you can load the “IK Test” Unity scene. This contains two useful objects, a Box, which is an instantiable container to for testing multiple arms; and a Manager, which can be used to instantiate and command your robot just as the task scenes would. To make use of this, simply add your new robot as an item in Robots field of the IK Test component. Then, for each robot in the list, it will instantiate a copy of the Box and ask the robot to follow the InputCommand transform. The InputCommand transform is then controlled by the enabled paths in the Box (only one should be enabled at a time). These will cyclically lerp/slerp between each child pose. If all works correctly, your robot should be moving along with the rest of them. This environment can be used to test the controller for robustness and accuracy. Additionally, it is often necessary to tweak the dynamic parameters of the robot models once imported to achieve good, stable, realistic results. ",
    "url": "http://localhost:4000/modification_workflow/new_robot.html#3-testing-the-control",
    "relUrl": "/modification_workflow/new_robot.html#3-testing-the-control"
  },"53": {
    "doc": "New Robot",
    "title": "New Robot",
    "content": " ",
    "url": "http://localhost:4000/modification_workflow/new_robot.html",
    "relUrl": "/modification_workflow/new_robot.html"
  },"54": {
    "doc": "New Task",
    "title": "Creating a New Task",
    "content": "Creating a new task is a relatively straightforward process, although it requires specifying several properties of the task. The simplest method is to copy and existing task and make modifications. However, the following guide highlights all of the necessary components to make a task from scratch. ",
    "url": "http://localhost:4000/modification_workflow/new_task.html#creating-a-new-task",
    "relUrl": "/modification_workflow/new_task.html#creating-a-new-task"
  },"55": {
    "doc": "New Task",
    "title": "1. Design the Task",
    "content": "The most important part of creating a new task is to have the necessary task components. This includes 3D models, textures, and any necessary new script components. It is often useful to visit the Unity Asset Store to find existing components or models. There are often many free assets that can be useful, and there are several paid assets that are worth considering if you have the budget. ",
    "url": "http://localhost:4000/modification_workflow/new_task.html#1-design-the-task",
    "relUrl": "/modification_workflow/new_task.html#1-design-the-task"
  },"56": {
    "doc": "New Task",
    "title": "2. Create the Right Structure",
    "content": "To be a properly formed task that supports all of the feedback and input modalities, there are a few things that need to be in place. As an example, we will walk through the ARAT task shown below in the Unity Prefab isolation editor view. Firstly, you need a new class that inherits from the Task component. If we take a look at the ComplexDexterityTask, we can see that this does not have any implementation requirements, but allows you to include task-level scripting if necessary. This task only uses these hooks to start and stop a clock. public class ComplexDexterityTask : Task { public Clock clock; void Start() { clock.StartTimer(() =&gt; { Pause(); }, 240); } public override void OnEnable() { base.OnEnable(); clock.StartStopwatch(); } } . In addition to new class, the Task component requires specification of: . | A robot start position transform (RobotPosition) | The commanded pose start position where the robot will place its end-effector initially (StartPostion) | The end-effector model that the robot arm will use (End Effector) | A timer that is referenced inside of the Task script | A list of cameras placed in the scene (Scene Cameras) | A set of task configurations that correspond to semantic difficulty levels (Task Configurations) . | Accompanied with a difficulty map that takes a SystemManager.Difficulty enum value and maps it to a parent game object. | . | . If we take a look at the object hierarchy and the component inspector, we can see the variables and configuration that is required. | GameObject Hierarchy | Component Inspector | . | | | . If all of these components are present, then all of the input and feedback systems should function as well. Optionally, to enable task completion display, you should include a SubTaskCompletionDisplay component in your scene. This will automatically look for active sub tasks in the scene and display their progress. ",
    "url": "http://localhost:4000/modification_workflow/new_task.html#2-create-the-right-structure",
    "relUrl": "/modification_workflow/new_task.html#2-create-the-right-structure"
  },"57": {
    "doc": "New Task",
    "title": "3. Create Your Task Prefab",
    "content": "Once you have syntactically complete task, you need to create a new prefab or place the existing prefab into the Resources folder. This is critical to enable task logging and replay, as the TaskRecord component needs to be able to find the task in the folder hierarchy. ",
    "url": "http://localhost:4000/modification_workflow/new_task.html#3-create-your-task-prefab",
    "relUrl": "/modification_workflow/new_task.html#3-create-your-task-prefab"
  },"58": {
    "doc": "New Task",
    "title": "4. Add the Task to the Menu",
    "content": "Lastly, to enable the task for user selection, head over to the main scene and expand the Hub-&gt;Canvas-&gt;Scene gameobject. The Scene object contains a SceneSelector component that maps the button to the correct task prefab object. Copy one of the existing buttons and configure it to your liking. You can even create a Unity Sprite out of an existing image file to use as a thumbnail image. Once you have created the button, link it to the appropriate prefab by adding an entry in the Scene Selector Map property. Now your task should be accessible from the main menu and it should support all of the configurations and modalities. ",
    "url": "http://localhost:4000/modification_workflow/new_task.html#4-add-the-task-to-the-menu",
    "relUrl": "/modification_workflow/new_task.html#4-add-the-task-to-the-menu"
  },"59": {
    "doc": "New Task",
    "title": "New Task",
    "content": " ",
    "url": "http://localhost:4000/modification_workflow/new_task.html",
    "relUrl": "/modification_workflow/new_task.html"
  },"60": {
    "doc": "Rectangle Packing",
    "title": "Usage",
    "content": "The RectanglePacking class in the Utils namespace provides a simple method for arranging a list of rectangles within a larger rectanlge. This is not a particularly efficient algorithm, so it is recommended to generate a single RectangleArrangement for a give configuration and reuse it. To generate an arrangement, all you need to do it call RectanglePacking.arrangeRectangles with a list of dimensions of the rectangles and a desired collage aspect ratio. You will then receive a RectangleArrangement struct that contains the total size of the arrangement (in the same units used to specify the original rectangles) and the absolute positions of each rectangle. Optionally, you can specify the horizontal and vertical justification to determine the placement when empty space is unavoidable. public class RectangleArrangement { public Vector2 dimensions; public List&lt;(Vector2, Vector2)&gt; children; // Top-left and bottom-right coordinates of the arranged rectangle public float fillRatio() { return children.Sum(x =&gt; x.Item2.x * x.Item2.y) / (dimensions.x * dimensions.y); } } . The list of rectangles has the same length of the list provided, and each return rectangle represents the placement position of the input rectangle at the same index. ",
    "url": "http://localhost:4000/behind_the_scenes/components/rectangle_packing.html#usage",
    "relUrl": "/behind_the_scenes/components/rectangle_packing.html#usage"
  },"61": {
    "doc": "Rectangle Packing",
    "title": "Techincal Details",
    "content": "Generating an optimal rectangle packing is in general an NP-hard problem. The algorithm implemented here scales very poorly, and in practice becomes impractical after ~30 rectangles. To make it manageable, several assumptions are made: . | Rectangles cannot be rotated. | Rectangles cannot be scaled. | The order in which the rectangles are provided is the order in which they should appead (from top-left to bottom-right). | . With these assumptions, it seeks to break the input list into rows that result in the “best packed” configuration, as measured by the fraction of unfilled space in the resulting arrangement. With this goal, it simply tries all of the allowed configurations and chooses the best one. public static RectangleArrangement Arrange(List&lt;Vector2&gt; rectangles, float outAspect, int rows = -1, int cols = -1, Justification hJustification = Justification.Left, Justification vJustification = Justification.Left) { float bestRatio = -1; RectangleArrangement bestArrangement = null; // Iterate over all of the possible combinations // Becuase of assumption 1,2, and 3, simply specifying the number of rectangles // uniquely specifies the arrangement. foreach (List&lt;int&gt; rowSizes in possibleRows(rectangles.Count, rows, cols)) { // For each one, generate the specific rectangle arrangement // This computes the top left points as well as the width and height of each rectangle // as well as the dimensions of the minimum bounding rectangle RectangleArrangement arrangement = arrangeRectangles( placeRectangles(rowSizes, rectangles), outAspect, hJustification, vJustification ); // If this is a good fill ratio, then store it as the best // arrangement float ratio = arrangement.fillRatio(); if (ratio &gt; bestRatio) { bestArrangement = arrangement; bestRatio = ratio; } } return bestArrangement; } . ",
    "url": "http://localhost:4000/behind_the_scenes/components/rectangle_packing.html#techincal-details",
    "relUrl": "/behind_the_scenes/components/rectangle_packing.html#techincal-details"
  },"62": {
    "doc": "Rectangle Packing",
    "title": "Rectangle Packing",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/rectangle_packing.html",
    "relUrl": "/behind_the_scenes/components/rectangle_packing.html"
  },"63": {
    "doc": "Robot Arms",
    "title": "Robot Models",
    "content": "This project focuses on the dexterity of human + robot systems on a table-top scale with robot arms. When comparing which robot arms are effective for teleoperation, the two main factors to consider are the kinematic and dynamic profiles. ",
    "url": "http://localhost:4000/task_models/robot_models.html#robot-models",
    "relUrl": "/task_models/robot_models.html#robot-models"
  },"64": {
    "doc": "Robot Arms",
    "title": "Kinematics",
    "content": "In simulation, the most obvious challenge when controlling a robot arm is generating good solutions to the inverse kinematics problem for control. The performance of a human + robot teleoperation system is highly dependent on the ability to generate feasbile and smooth robot motions from human inputs. And in turn, the ability to generate such motions can depend on subtleties of the particular robot kinematics1. ",
    "url": "http://localhost:4000/task_models/robot_models.html#kinematics",
    "relUrl": "/task_models/robot_models.html#kinematics"
  },"65": {
    "doc": "Robot Arms",
    "title": "Dynamics",
    "content": "When simulating a robot arm, it is possible to completely ignore the dynamic constraints by generating un-physical motions. While this choice simplifies the system significantly, it is often the case that robot arms with more favorable kinematics have stricter dynamic constraints and ignoring them would lead one to conclude that certain arms are better than they are for a real task. This project aims to both offer a reasonable facsimile of real robot dynamics as well as the option to change the dynamics to explore the effect on task performance. ",
    "url": "http://localhost:4000/task_models/robot_models.html#dynamics",
    "relUrl": "/task_models/robot_models.html#dynamics"
  },"66": {
    "doc": "Robot Arms",
    "title": "Included Robots",
    "content": "Franka Emika Panda . Kuka iiwa7 . Sawyer . Baxter . UR5 . | A thorough description of the complexities of robot inverse kinematics can be found here. &#8617; . | . ",
    "url": "http://localhost:4000/task_models/robot_models.html#included-robots",
    "relUrl": "/task_models/robot_models.html#included-robots"
  },"67": {
    "doc": "Robot Arms",
    "title": "Robot Arms",
    "content": " ",
    "url": "http://localhost:4000/task_models/robot_models.html",
    "relUrl": "/task_models/robot_models.html"
  },"68": {
    "doc": "Sticky Gripper",
    "title": "Usage",
    "content": "The Stick Gripper component can be attached to a robot end effector and receive end effector activation triggers. To function correctly, this component requires that you have one or more fingers listed. Each finger needs to have an Articulation Body specified, either explicitly or present on the component. Similarly, the Detector can either be set explicitly to a Box Collider component, or it will automatically look for one on the finger. The detector is needed to determine whether or not a finger is in graspable contant with an object. When the gripper is activated, the Gripper Lower Limit and Gripper Upper Limit variables are used along with the activation level ([0, 1]), to determine what command should be sent to the fingers. ",
    "url": "http://localhost:4000/behind_the_scenes/components/sticky_gripper.html#usage",
    "relUrl": "/behind_the_scenes/components/sticky_gripper.html#usage"
  },"69": {
    "doc": "Sticky Gripper",
    "title": "Technical Details",
    "content": "When interacting with the environment, it is typically not sufficient to rely on to grab and manipulate things. Opposing fingers that are constantly in contact with an object will require the physics engine to maintain tight position margins to prevent object clipping. The lack of necessary precision in this respect means that the friction physics options aren’t consistent and allow for frequent slipping and/or penetration1. The StickyGripper and Finger components were designed to improve this behavior. A StickyGripper keeps track of Rigidbodys that are being held, and connects them to the gripper via FixedJoint components. When the object is no longer being held, the FixedJoint is removed. While an object is being held, it is removed from Unity’s dynamics pipeline by setting it to be a kinematic Rigidbody. The StickyGripper relies on its Finger child components. The Finger component represents an articulable finger joint that can detect whether or not its in contact with an object. Each Finger contains a Trigger Collider that detects intesecting objects with the OnTriggerStay and OnTriggerExit methods. Because some gripper models contain multiple objects, links or joints, there is also the FingerPad component that can hold the contact detector and forward it’s contants to the main Finger. | | . | Shown is a single finger from the Franka Emika Panda gripper. The finger has normal colliders that coincide with the visual representation. However, the wireframe box on the finger pad shows the trigger collider that is used for grasp detection. | . At each FixedUpdate, the StickyGripper determines which objects are in contact with all fingers, and holds onto objects that are while dropping objects that are no longer in contact. | The physics materials are already configured for the end-effectors. To experience this for yourself, simply disable the StickyGripper component. &#8617; . | . ",
    "url": "http://localhost:4000/behind_the_scenes/components/sticky_gripper.html#technical-details",
    "relUrl": "/behind_the_scenes/components/sticky_gripper.html#technical-details"
  },"70": {
    "doc": "Sticky Gripper",
    "title": "Sticky Gripper",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/sticky_gripper.html",
    "relUrl": "/behind_the_scenes/components/sticky_gripper.html"
  },"71": {
    "doc": "Painting",
    "title": "Surface Painting",
    "content": "Writing directly to a texture based on spatial proximity is not something that is available in Unity by default. This is where the Canvas and Marker components come in. ",
    "url": "http://localhost:4000/behind_the_scenes/components/surface_painting.html#surface-painting",
    "relUrl": "/behind_the_scenes/components/surface_painting.html#surface-painting"
  },"72": {
    "doc": "Painting",
    "title": "Canvas",
    "content": "The DrawableMesh component exposes two operations: Clear, which removes all current markings on the texture, and Draw, which adds new markings to the mesh. The Draw function takes a 3D capsule in world-space and compares it the mesh, adding the specified color over the texture wherever they intersect. This effect is achieved by maintaining 3 separate textures with a custom surface shader to layer the marker with the original texture, and a custom unlit shader that checks each pixel for intersection with the world-space capsule to determine whether or not it should be marked. Unity Shaders . A “shader” in Unity is simply a program that is compiled for and run on the GPU. A shader can be attached to a material, and is used to determine the color and lighting properties of a specific point on the surface of a mesh. The Unity shader infrastruture is set up to minimize the amount of work that is required of developers. For a simple surface shader, all that is needed is to create a function that maps the UV cooridnates to a struct of color properties. An example can be seen in the following section. Canvas.shader . This custom shader is a simple surface shader that performs an alpha blending to layer the marker on top of the original mesh texture1. The following is an excerpt from Canvas.shader with the boilerplate code elided: ... sampler2D _MainTex; sampler2D _Marker; struct Input { float2 uv_MainTex; }; ... void surf (Input IN, inout SurfaceOutputStandard o) { // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; fixed4 marker_color = tex2D(_Marker, IN.uv_MainTex); o.Albedo = marker_color.rgb * marker_color.a + c.rgb * (1 - marker_color.a); // Metallic and smoothness come from slider variables o.Metallic = _Metallic * (1 - marker_color.a); o.Smoothness = _Glossiness * (1 - marker_color.a); o.Alpha = marker_color.a * marker_color.a + c.a * (1 - marker_color.a); } ... In this example, the function surf takes a UV coordinate as an input, in the form of an Input struct... struct Input { float2 uv_MainTex; }; ... void surf (Input IN ..... It then looks up the color of the original texture as well as the marker texture to overlay, blends them together, and stores the result in the output parameter, ..., inout SurfaceOutputStandard o)... sampler2D _MainTex; sampler2D _Marker; ... // Albedo comes from a texture tinted by color fixed4 c = tex2D (_MainTex, IN.uv_MainTex) * _Color; fixed4 marker_color = tex2D(_Marker, IN.uv_MainTex); o.Albedo = marker_color.rgb * marker_color.a + c.rgb * (1 - marker_color.a); // Metallic and smoothness come from slider variables o.Metallic = _Metallic * (1 - marker_color.a); o.Smoothness = _Glossiness * (1 - marker_color.a); o.Alpha = marker_color.a * marker_color.a + c.a * (1 - marker_color.a); ... The thread-global parameters _MainTex and _Marker are set by the Unity material object and the DrawableMesh component, respectively. // Start is called before the first frame update void Start() { // Read the texture attached to our gameObject. Texture mainTex = GetComponent&lt;MeshRenderer&gt;().material.mainTexture; // Create a new blank texture to hold the marker layer. MarkerTex = new Texture2D(1024, 1024, TextureFormat.RGBAFloat, false); fill(MarkerTex, new Color(0,0,0,0)); ... // Give the shader a pointer to the marker texture. GetComponent&lt;MeshRenderer&gt;().material.SetTexture(\"_Marker\", MarkerTex); } . | Marker Texture | Main Texture | Rendered Result | . | | | | . ",
    "url": "http://localhost:4000/behind_the_scenes/components/surface_painting.html#canvas",
    "relUrl": "/behind_the_scenes/components/surface_painting.html#canvas"
  },"73": {
    "doc": "Painting",
    "title": "Creating the Marker Overlay",
    "content": "To determine whether or not a pixel should be marked, we need to generate a map from UV space to world-space2. To do this, we are going to create a texture the same size as _Marker that will hold the 3D coordinates of each point in the RGB fields3. This approach requires rasterizing the mesh to the texture, using the UV coordinates of each vertex instead of its world coordinates. To avoid writing a custom CPU rasterization algorithm and make use of Unity’s render pipeline, we will opt to create a mesh in the unity world and take a picture of it using an orthographic camera. | World Space Cup Model | UV Unwrapped, Position Colored Mesh | . | | | . RasterizeUV Function . To generate the colored mesh, we only need to loop through all of the vertices of the current mesh and make a new one with the vertex positions equal to the UV positions, and the color equal to the vertex position. We begin by looping through the vertices to create an axis aligned bounding box specified by the minimum corner and the size along each axis. For convenience later, we will store the inverse of the size in a vector as scale. private void RasterizeUV() { var mesh = GetComponent&lt;MeshFilter&gt;().mesh; float[] min = new float[]{float.MaxValue, float.MaxValue, float.MaxValue}; float[] max = new float[]{float.MinValue, float.MinValue, float.MinValue}; foreach (Vector3 v in mesh.vertices) { for (int i = 0; i &lt; 3; i++) { min[i] = Mathf.Min(min[i], v[i]); max[i] = Mathf.Max(max[i], v[i]); } } Vector3 lower = new Vector3(min[0], min[1], min[2]); Vector3 scale = new Vector3( 1 / (0.000001f + max[0] - min[0]), 1 / (0.000001f + max[1] - min[1]), 1 / (0.000001f + max[2] - min[2]) ); ... Next, we create a list of vertices from the UV coordinates. Here, mesh.uv stores the uv coordinates for each vertex4. We will choose z = 0 for simplicity. We then loop again to set the color of our new vertices. The Color struct stores rgba values as floats in [0, 1], so we transform our object coordinates to normalized coordinates using the bounding box we computed previously. Finally, we store a transformation matrix that maps normalized coordinates back to object coordinates to use in the shader later... // Set the positions float aspectRatio = ((float)MarkerTex.width) / MarkerTex.height; Vector3[] newVertices = new Vector3[mesh.uv.Length]; for (int i = 0; i &lt; newVertices.Length; i++) { newVertices[i] = new Vector3(mesh.uv[i].x * aspectRatio, mesh.uv[i].y, 0); } // Set the colors Color[] colors = new Color[newVertices.Length]; for (int i = 0; i &lt; colors.Length; i++) { Vector3 c = mesh.vertices[i] - lower; c.Scale(scale); colors[i] = new Color(c.x, c.y, c.z, 1); } NormalizedToObj = Matrix4x4.TRS( lower, Quaternion.identity, new Vector3(max[0] - min[0], max[1] - min[1], max[2] - min[2]) ); ... Now that we have the vertices and colors correct, we just need to create and render our mesh. For this, we will create two gameobjects: a camera and the UV mesh. To ensure the color of the new mesh is not affected by lighting, we assign an unlit material (called UVRaster here) to it. To ensure this camera only sees our new mesh, we create a Layer in the Unity Editor, assign it to our mesh, and tell the camera to only render that layer. Additionally, we will disable the camera script to prevent it from rendering every frame, since we only want to take a single picture, and assign a Render Texture as the target to render to... GameObject raster_obj = new GameObject(); raster_obj.AddComponent&lt;MeshFilter&gt;().mesh = new Mesh(); var uv_mesh = raster_obj.GetComponent&lt;MeshFilter&gt;().mesh; uv_mesh.vertices = newVertices; uv_mesh.colors = colors; uv_mesh.triangles = mesh.triangles; // Mesh postprocessing uv_mesh.RecalculateNormals(); uv_mesh.RecalculateTangents(); // Create the game object and assign our raster_obj.layer = LayerMask.NameToLayer(\"UVRaster\"); raster_obj.AddComponent&lt;MeshRenderer&gt;(); raster_obj.GetComponent&lt;MeshRenderer&gt;().material = UVRaster; GameObject cam_obj = new GameObject(); cam_obj.transform.localPosition = new Vector3(aspectRatio / 2, 0.5f, -1); cam_obj.AddComponent&lt;Camera&gt;(); Camera cam = cam_obj.GetComponent&lt;Camera&gt;(); cam.orthographic = true; cam.orthographicSize = 0.5f; cam.cullingMask = LayerMask.GetMask(\"UVRaster\"); cam.enabled = false; cam.targetTexture = new RenderTexture(MarkerTex.width, MarkerTex.height, 0, RenderTextureFormat.ARGBFloat); cam.clearFlags = CameraClearFlags.SolidColor; cam.backgroundColor = new Color(0,0,0,0); ... Finally, we ask the camera to render to the texture. Since the Render Texture buffer data is stored on the GPU, we will create a new Texture2D and copy the data from the Render Texture into that. Then we can clean up by destroying both the camera and the UV mesh. We will also go ahead and set the _UVPosition parameter of our shader so it has it when it comes time to draw... cam.Render(); uvPositionTexture = new Texture2D(MarkerTex.width, MarkerTex.height, TextureFormat.RGBAFloat, false); RenderTexture.active = cam.targetTexture; uvPositionTexture.ReadPixels(new Rect(0, 0, MarkerTex.width, MarkerTex.height), 0, 0); uvPositionTexture.Apply(); RenderTexture.active = null; DrawMat.SetTexture(\"_UVPosition\", uvPositionTexture); Destroy(raster_obj); Destroy(cam_obj); Initialized = true; } . Draw Function . Now that we have rasterized the UV mesh onto a texture, we have all of the peices we need to start drawing. The simplest thing to draw is a single point with a specified radius. However, we may not be calling this function with a super high frequency, it allows the caller to specify a value for lastDrawPoint to interpolate from. This amounts to intersecting the mesh with a cylinder specified by a line segment and a radius. public void Draw(Vector3 drawPoint, float radius, Color drawColor, Vector3? lastDrawPoint = null) { if (!Initialized) return; ... The DrawMatShader shader is going to need several parameters, so we set those here. The radius, color, and line segment are taken as inputs from the caller, but we also will pass it a transformation matrix to transform the coordinates stored in the color values of the _UVPosition texture into world coordinates based on our object’s current pose... DrawMat.SetColor(\"_DrawColor\", drawColor); DrawMat.SetFloat(\"_Radius\", radius); DrawMat.SetMatrix( \"_NormalizedToWorld\", Matrix4x4.TRS(transform.position, transform.rotation, transform.localScale) * NormalizedToObj ); DrawMat.SetVector(\"_DrawPoint\", new Vector4(drawPoint.x, drawPoint.y, drawPoint.z, 0)); Vector3 last = lastDrawPoint.GetValueOrDefault(drawPoint); DrawMat.SetVector(\"_LastDrawPoint\", new Vector4(last.x, last.y, last.z, 0)); ... Then, the actual work is done by using Unity’s Blit function to update our current marker overlay with any new marks. Then we can simply read the result back into the marker overlay texture, knowing that it will get rendered over our base material texture when a camera is looking at it... RenderTexture tmp = RenderTexture.GetTemporary(MarkerTex.width, MarkerTex.height); // This first call to Blit stores the current marker overlay texture // into tmp Graphics.Blit(MarkerTex, tmp); // Then we invoke DrawMatShader on the uvPositionTexture with the parameters // we just set, and store the result in tmp as well. // By passing a material with our shader attached into Blit, it will run that // shader Graphics.Blit(uvPositionTexture, tmp, DrawMat); RenderTexture.active = tmp; MarkerTex.ReadPixels(new Rect(0, 0, MarkerTex.width, MarkerTex.height), 0, 0); MarkerTex.Apply(); RenderTexture.active = null; RenderTexture.ReleaseTemporary(tmp); } . DrawMatShader Fragment Shader . The final piece of code here is the shader that checks each pixel in the marker texture to see if it should be marked or not. There are two things to call out here. Firstly, this shader uses an alpha blend option of ... Blend SrcAlpha OneMinusSrcAlpha ... This just ensures that we don’t forget about old markings on the marker overlay when we Blit the new marks onto it. Secondly, the actual code that is being run for each pixel: ... float4 frag (v2f i) : SV_Target { if (_Radius &lt; 0) return float4(0,0,0,0); float r = _Radius * _Radius; float3 pos = tex2D(_UVPosition, i.uv).rgb; pos = mul(_NormalizedToWorld, float4(pos, 1)).xyz; float3 a = pos - _DrawPoint; float3 b = pos - _LastDrawPoint; if (dot(a, a) &lt;= r || dot(b, b) &lt;= r) { return _DrawColor; } float3 n = _LastDrawPoint - _DrawPoint; float l = sqrt(dot(n,n)); n = n / l; float projectedDistance = dot(a, n); if (projectedDistance &gt;= 0 &amp;&amp; projectedDistance &lt;= l &amp;&amp; dot(a, a) - projectedDistance * projectedDistance &lt;= r) { return _DrawColor; } return float4(0,0,0,0); } ... In this function, we extract the position from the texture color, transform it from normalized to world coordinates, and check the distance to the line segment to see if that pixel should be the marker color, or transparent. ",
    "url": "http://localhost:4000/behind_the_scenes/components/surface_painting.html#creating-the-marker-overlay",
    "relUrl": "/behind_the_scenes/components/surface_painting.html#creating-the-marker-overlay"
  },"74": {
    "doc": "Painting",
    "title": "Marker",
    "content": "The Marker component serves only to trigger Draw calls to DrawableMeshes in the scene. To do this, it has a trigger collider that represents the marker “tip”. When it intersects with a DrawableMesh, it draws on the mesh at the closest point to the tip. void OnTriggerStay(Collider other) { DrawableMesh dm = other.gameObject.GetComponent&lt;DrawableMesh&gt;(); if (dm != null) { if (!MeshDrawHistory.ContainsKey(dm)) { MeshDrawHistory.Add(dm, null); } Vector3 point = other.ClosestPoint(Tip.position); Vector3 last = MeshDrawHistory[dm].GetValueOrDefault(point); MeshDrawHistory[dm] = point; dm.Draw(point, Size, DrawColor, last); } } . | This is also possible to do by adding a second material to the mesh and using Unity’s built in texture blending options. &#8617; . | Shader code gets called for every pixel in the camera that is occupied by our object. To determine which things are visible, Unity already maps world-space coords to UV coords and you can receive it as an additional parameter in your shader function. So, it might be tempting to try to use the already known values. However, this proves difficult, since the resolution and whether or not the function is even called is determined by the camera position and parameters, which means things wouldn’t be drawn if you weren’t looking at them. Also, since you are not allowed to write global data in a shader thread, you can’t accumulate markings. So, we are out of luck and need to do it ourselves. &#8617; . | Overloading texture color data is a common way to do other things with shaders than render colors. &#8617; . | Unity Meshs can have multiple sets of UV coordinates. Loading uvs to the wrong channel when importing, or not explicitly generating them at all when making the mesh would cause this code to fail. &#8617; . | . ",
    "url": "http://localhost:4000/behind_the_scenes/components/surface_painting.html#marker",
    "relUrl": "/behind_the_scenes/components/surface_painting.html#marker"
  },"75": {
    "doc": "Painting",
    "title": "Painting",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/surface_painting.html",
    "relUrl": "/behind_the_scenes/components/surface_painting.html"
  },"76": {
    "doc": "Tracing",
    "title": "Tracing",
    "content": "Another manipulability benchmark that we considered fundamental was controlled, precise surface interaction. The ability to interact within a constrained task space that affords fewer dimensions than the input device is common in a wide range of applications, including sanding, painting, and composite lay-up. Additionally, tracing tests have been effectively used to asses human dexterity as well as teleoperation dexterity. To objectively measure the quality of the trace, we look at the number of pixels that are correctly painted in and compare that to the number of pixels that are incorrectly painted in. This score can vary significantly depending on brush stroke size and target image resolution, but can be used to consistently compare two different control configurations with the same tracing parameters. ",
    "url": "http://localhost:4000/task_models/tracing.html",
    "relUrl": "/task_models/tracing.html"
  },"77": {
    "doc": "Tracing",
    "title": "Technical Details",
    "content": "The tracing task is enabled by two components, a Marker end-effector and a DrawableMesh canvas. For details on how this was created, see the Painting component page. In addition to the painting component, there is also a scoring metric contained in the SurfacePaintingSubTask component. This metric uses a source image (the cursive “lab” in the above figure) and generates a score map. When evaluating the user’s performance, marker overlay is compared against the score map and the user recieves either positive or negative points for each pixel they have marked. To efficiently compute this, the score map is generated and then used in the context of a Unity Compute Shader, which enables general purpose GPU computation. At design time, the Signed Distance Field of the binary image is generated using the SDF-Toolkit (Free) Unity Plugin. | Original Stencil | SDF (Whiter is Closer) | . | | . Then, the value of a mark is accumulated by multiplying the alpha channel of the mark with the score for that particular pixel generated from the SDF. SDF values equal to 0 have a score multiplier of 1, while SDF values greater than 0 have a negative score multiplier that scales with distance to the curve. The TracingSubTask component contains definitions for ParallelScorer and ParallelSum objects. Each rely on the ScoreTracing.compute shader definition, each of which manage the memory allocation for their respective GPU kernels. ",
    "url": "http://localhost:4000/task_models/tracing.html#technical-details",
    "relUrl": "/task_models/tracing.html#technical-details"
  },"78": {
    "doc": "Tracing",
    "title": "ParallelScorer",
    "content": "This object implements slightly modified component-wise matrix multiplication. The alpha channel of the Texture2D ScoreMap holds the SDF for the curve, which is mapped to a score multiplier via a simple scoring function. void Score(uint id: SV_DispatchThreadID) { uint2 xy = uint2(id / res_y, id % res_x); float a = ScoreMap[xy][3]; float score = a &gt; 0.99 ? 1 : a-1; if (id &lt; res_x * res_y) Result[id] = score * Marking[xy][3]; } . ",
    "url": "http://localhost:4000/task_models/tracing.html#parallelscorer",
    "relUrl": "/task_models/tracing.html#parallelscorer"
  },"79": {
    "doc": "Tracing",
    "title": "ParallelSummer",
    "content": "This object implements a standard parallel sum reduction, and is used to accumulate the results of the ParallelScorer. The included shader and scoring function is an implementation of the Blelloch modification to the Hillis and Steele parallel scan operation, except without the down sweep operation. groupshared float bucket[THREADS_PER_GROUP]; void Scan(uint id, uint gi, uint gnumber, float x) { bucket[gi] = x; [unroll] for (uint t = 1; t &lt; THREADS_PER_GROUP; t &lt;&lt;= 1) { GroupMemoryBarrierWithGroupSync(); float temp = bucket[gi]; uint right_index = gi + t; if (right_index &lt; THREADS_PER_GROUP) temp += bucket[right_index]; GroupMemoryBarrierWithGroupSync(); bucket[gi] = temp; } if (gi == 0) OutputBuf[gnumber] = bucket[gi]; } uint length; #pragma kernel Sum [numthreads(THREADS_PER_GROUP, 1, 1)] void Sum(uint id: SV_DispatchThreadID, uint gid : SV_GroupIndex, uint gnumber : SV_GroupID) { float x = id &lt; length ? InputBuf[id] : 0; Scan(id, gid, gnumber, x); } . One invokation of the above Sum reduction call will produce a new array with the length reduced by a factor of THREADS_PER_GROUP. Using a typical value of 512, it is necessary to call this function multiple times in a row (3-4, usually) to accumulate the total result. ",
    "url": "http://localhost:4000/task_models/tracing.html#parallelsummer",
    "relUrl": "/task_models/tracing.html#parallelsummer"
  },"80": {
    "doc": "Tracker",
    "title": "Usage",
    "content": "The Tracker component is straightforward to use. Simply attach it to the object that should track another transform, and tell it which transform to track. The difference between this adn simply making one object a parent of another is the ability to introduce latency and low-pass filtering. | | . | Example of the tracker component on a robot’s IKTarget. The IKTarget tracks the user input with configurable dynamic constraints. | . ",
    "url": "http://localhost:4000/behind_the_scenes/components/tracker.html#usage",
    "relUrl": "/behind_the_scenes/components/tracker.html#usage"
  },"81": {
    "doc": "Tracker",
    "title": "Technical Details",
    "content": "At a fixed interval, the tracking object will read the current pose of the tracked transform and interperet it as a pose command. This command then goes through two stages of processing. First, it is placed in a buffer along with the current timestamp. Then, older items are removed from the buffer by comparing their timestamps with the current timestamp and the desired latency. latencyBuffer.Enqueue((time + Latency_ms / 1000, Target.position, Target.rotation)); while (latencyBuffer.Count &gt; 0 &amp;&amp; latencyBuffer.Peek().Item1 &lt;= time) { (float t, Vector3 position, Quaternion rotation) = latencyBuffer.Dequeue(); updateTransform(t, position, rotation); } . Secondly, the command is actually received and applied to the current transform. However, this is not before modifying it with a low-pass filter. float dt = Time.fixedDeltaTime; float gain = dt / (dt + 1 / (LowpassFilter)); ... transform.position = Vector3.Lerp(transform.position, position + OffsetPosition, gain); transform.rotation = Quaternion.Slerp( transform.rotation, rotation * Quaternion.Euler(OffsetRotation.x, OffsetRotation.y, OffsetRotation.z), gain ); . Lastly, the tracking component can be temporarily disabled. During this time, instead of processing the input commands, the OffsetPosition and OffsetRotation are modified to keep the tracking object stationary while the tracked object changes pose. ",
    "url": "http://localhost:4000/behind_the_scenes/components/tracker.html#technical-details",
    "relUrl": "/behind_the_scenes/components/tracker.html#technical-details"
  },"82": {
    "doc": "Tracker",
    "title": "Tracker",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/components/tracker.html",
    "relUrl": "/behind_the_scenes/components/tracker.html"
  },"83": {
    "doc": "URDF Importer",
    "title": "URDF Importer",
    "content": " ",
    "url": "http://localhost:4000/behind_the_scenes/modified_components/urdf_importer.html",
    "relUrl": "/behind_the_scenes/modified_components/urdf_importer.html"
  },"84": {
    "doc": "URDF Importer",
    "title": "AssimpNet",
    "content": "The Unity-Robotics-Hub is a repository for creating robotic simulations in Unity. As a sub-component, it includes URDF Importer. The primary function of the importer is to take a URDF file along with a robot description (visual meshs, collision meshes, and other collision geometry) and create a Unity object with the appropriate visual representation and articulation. This package was not created to work with WebGL builds. When building for a WebGL target, the Unity Editor attempts to include two native DLLs with conflicting names in the build (/Runtime/UnityMeshImporter/Plugins/AssimpNet/Native/win/\\[x86/x86_64\\]/assimp_x86.dll). To resolve the build error, I simply removed the dll that my system was not using (.../x86/assimp_x86.dll). ",
    "url": "http://localhost:4000/behind_the_scenes/modified_components/urdf_importer.html#assimpnet",
    "relUrl": "/behind_the_scenes/modified_components/urdf_importer.html#assimpnet"
  },"85": {
    "doc": "URDF Importer",
    "title": "Capsules",
    "content": "Capsule models were not originally supported in the URDF spec, and are not consistently supported now. Originally, URDF Importer also only supported Meshes, Boxes, Cylinders, and Spheres as geometries. To support the visualization and debugging of capsule colliders used for the IK Solver, small modifications were made to this Unity package to support the loading and generation of capsule colliders. ",
    "url": "http://localhost:4000/behind_the_scenes/modified_components/urdf_importer.html#capsules",
    "relUrl": "/behind_the_scenes/modified_components/urdf_importer.html#capsules"
  }
}
